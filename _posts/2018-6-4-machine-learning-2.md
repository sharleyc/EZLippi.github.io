---
layout:     post
title:      Machine Learning基础系列（二）
keywords:   博客
categories: [机器学习]
tags:	    [逻辑回归，正则化]
---

本文是机器学习第三周的课程内容。主要学习逻辑回归。逻辑回归是一种将数据分类为离散结果的方法。比如，我们可以使用逻辑回归将电子邮件分类为垃圾邮件或正常邮件。主要内容除了分类的概念，逻辑回归的代价函数，以及逻辑回归在多分类中的应用，还覆盖了正则化的概念。


## 分类      

我们先重点讨论二元分类问题，即y只能取两个值0和1，0也被称为负类，1是正类，有时也用符号“-”和“+”表示。要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，其它所有预测值映射为0。比如预测肿瘤是否是恶性还是良性的问题：   

  ![](/images/images_2018/6-4_0.png)       

我们的预测值只有在[0,1]才有意义，线性分类器则可能小于0或大于1。我们希望假设函数的预测值要在0和1之间，因此使用了S型函数（Sigmoid Function）或逻辑函数（Logistic Function）：    

 ![](/images/images_2018/6-4_1.png)      

从S型函数的图形看，它随着横坐标的正方向趋于1，随着横坐标的反方向趋于0。这个假设函数的输出值是对于输入样本x的y=1的概率的估计值。比如，以肿瘤分类为例，特征变量x是肿瘤的大小，假设x带入假设函数的输出为0.7，即肿瘤是恶性的可能性是70%或0.7。由于y必须是0或1，因此我们知道y=0的概率加上y=1的概率必须等于1。     

 ![](/images/images_2018/6-4_2.png)      


## 决策边界   

 ![](/images/images_2018/6-4_3.png)    

我们会注意到S函数只要z大于或等于0时，g(z)就将大于或等于0.5。如果我们默认预测y=1的概率大于或等于0.5时，y更有可能等于1而不是0，那么也意味着：   

 ![](/images/images_2018/6-4_4.png)    

决策边界将整个平面分成了两部分，其中一片区域假设函数预测y=1，另一片区域假设函数预测y=0。决策边界是假设函数的一个属性，一旦有确定的假设函数的参数，就能完全确定决策边界。    

  ![](/images/images_2018/6-4_5.png)   

在这个例子中决策边界是一条竖线，左边的表示y=1，右边的则表示y=0。 决策边界有时并不是一条直线，它可以是任意形状的，比如圆形。

   ![](/images/images_2018/6-4_6.png)    

## 代价函数    
 
我们不能使用和线性回归相同的代价函数，因为逻辑函数不是一个凸函数，会导致许多局部最优。但是以下面这种方式编写代价函数可以保证是凸函数。        

  ![](/images/images_2018/6-4_7.png)        

  ![](/images/images_2018/6-4_8.png)     

  ![](/images/images_2018/6-4_9.png)     

y=0时，如果假设函数也输出0，则代价函数为0，如果假设函数接近1，则代价函数将接近无穷大；
y=1时，如果假设函数也输出1，则代价函数为0，如果假设函数接近0，则代价函数将接近无穷大。   

  ![](/images/images_2018/6-4_10.png)


## 简化的代价函数和梯度下降算法   

将y=0和y=1两种情况下的表达式压缩成一个表达式：   
 
  ![](/images/images_2018/6-4_11.png)        

完整的代价函数J的表达式：   

  ![](/images/images_2018/6-4_12.png)     

代价函数向量化的表达式：    
 
   ![](/images/images_2018/6-4_13.png)     

逻辑回归的梯度下降算法与线性回归中使用的算法相同，仍然需要同时更新theta中的所有值：   
 
   ![](/images/images_2018/6-4_14.png)        

向量化的实现方式：    

   ![](/images/images_2018/6-4_15.png)  

   
## 多类别分类问题（One-vs-all）    

我们通过一个例子来了解什么是多类别分类问题。现在我们有一个训练集，有三个类别，三角形表示y=1，方框表示y=2，叉叉表示y=3。我们将它分成三个二元分类问题。先从三角形的代表的类别1开始。类别1是正类，类别2和3是负类，我们要拟合出一个合适的逻辑回归分类器，接着将为类别2做同样的事，把方块类当做正样本，其它类别为负样本，这样找到第二个分类器，同样的方法得到第三个分类器。现在我们有了三个分类器，接下来要做的是训练它们。在三个分类器里输入x，选择一个让h最大的类别。    

   ![](/images/images_2018/6-4_16.png)    

现在我们可以扩展我们的定义，当我们有两个以上的类别时y={0,1,...n}，我们将问题分为n+1二元分类问题，在每一个分类问题中去预测'y'是其中一个成员的概率，并选择那个概率最大的类别。   
  
   ![](/images/images_2018/6-4_17.png)     


## 逻辑回归测验题        

   ![](/images/images_2018/6-4_18.png)     

答案：A、C。y=1的概率是0.4，那么y=0的概率就是0.6。

   ![](/images/images_2018/6-4_19.png)     
 
   ![](/images/images_2018/6-4_19.jpg)        

答案：A、B。增加多项式特征，可以更好的拟合训练集是正确的；代价函数大于等于0是正确的；增加多项式特征可以更好的拟合训练集，因此代价函数的值不会变大，而是变小；无论梯度下降算法迭代多少次，假设函数的值是不会大于1的。 

   ![](/images/images_2018/6-4_20.png)    

答案：B、C。  

   ![](/images/images_2018/6-4_21.png)    

答案： A、C。  

   ![](/images/images_2018/6-4_22.png)      

   ![](/images/images_2018/6-4_23.png)  

答案：A。  
