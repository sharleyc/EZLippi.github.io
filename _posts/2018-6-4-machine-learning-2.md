---
layout:     post
title:      Machine Learning基础系列（二）
keywords:   博客
categories: [机器学习]
tags:	    [逻辑回归，正则化]
---

本文是机器学习第三周的课程内容。主要学习逻辑回归。逻辑回归是一种将数据分类为离散结果的方法。比如，我们可以使用逻辑回归将电子邮件分类为垃圾邮件或正常邮件。主要内容除了分类的概念，逻辑回归的代价函数，以及逻辑回归在多分类中的应用，还覆盖了正则化的概念。


## 分类      

我们先重点讨论二元分类问题，即y只能取两个值0和1，0也被称为负类，1是正类，有时也用符号“-”和“+”表示。要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，其它所有预测值映射为0。比如预测肿瘤是否是恶性还是良性的问题：   

  ![](/images/images_2018/6-4_0.png)       

我们的预测值只有在[0,1]才有意义，线性分类器则可能小于0或大于1。我们希望假设函数的预测值要在0和1之间，因此使用了S型函数（Sigmoid Function）或逻辑函数（Logistic Function）：    

 ![](/images/images_2018/6-4_1.png)      

从S型函数的图形看，它随着横坐标的正方向趋于1，随着横坐标的反方向趋于0。这个假设函数的输出值是对于输入样本x的y=1的概率的估计值。比如，以肿瘤分类为例，特征变量x是肿瘤的大小，假设x带入假设函数的输出为0.7，即肿瘤是恶性的可能性是70%或0.7。由于y必须是0或1，因此我们知道y=0的概率加上y=1的概率必须等于1。     

 ![](/images/images_2018/6-4_2.png)      


## 决策边界   

 ![](/images/images_2018/6-4_3.png)    

我们会注意到S函数只要z大于或等于0时，g(z)就将大于或等于0.5。如果我们默认预测y=1的概率大于或等于0.5时，y更有可能等于1而不是0，那么也意味着：   

 ![](/images/images_2018/6-4_4.png)    

决策边界将整个平面分成了两部分，其中一片区域假设函数预测y=1，另一片区域假设函数预测y=0。决策边界是假设函数的一个属性，一旦有确定的假设函数的参数，就能完全确定决策边界。    

  ![](/images/images_2018/6-4_5.png)   

在这个例子中决策边界是一条竖线，左边的表示y=1，右边的则表示y=0。 决策边界有时并不是一条直线，它可以是任意形状的，比如圆形。

   ![](/images/images_2018/6-4_6.png)    

## 代价函数    
 
我们不能使用和线性回归相同的代价函数，因为逻辑函数不是一个凸函数，会导致许多局部最优。但是以下面这种方式编写代价函数可以保证是凸函数。        

  ![](/images/images_2018/6-4_7.png)        

  ![](/images/images_2018/6-4_8.png)     

  ![](/images/images_2018/6-4_9.png)     

y=0时，如果假设函数也输出0，则代价函数为0，如果假设函数接近1，则代价函数将接近无穷大；
y=1时，如果假设函数也输出1，则代价函数为0，如果假设函数接近0，则代价函数将接近无穷大。   

  ![](/images/images_2018/6-4_10.png)


## 简化的代价函数和梯度下降算法   

将y=0和y=1两种情况下的表达式压缩成一个表达式：   
 
  ![](/images/images_2018/6-4_11.png)        

完整的代价函数J的表达式：   

  ![](/images/images_2018/6-4_12.png)     

代价函数向量化的表达式：    
 
   ![](/images/images_2018/6-4_13.png)     

逻辑回归的梯度下降算法与线性回归中使用的算法相同，仍然需要同时更新theta中的所有值：   
 
   ![](/images/images_2018/6-4_14.png)        

向量化的实现方式：    

   ![](/images/images_2018/6-4_15.png)  

   
## 多类别分类问题（One-vs-all）    

我们通过一个例子来了解什么是多类别分类问题。现在我们有一个训练集，有三个类别，三角形表示y=1，方框表示y=2，叉叉表示y=3。我们将它分成三个二元分类问题。先从三角形的代表的类别1开始。类别1是正类，类别2和3是负类，我们要拟合出一个合适的逻辑回归分类器，接着将为类别2做同样的事，把方块类当做正样本，其它类别为负样本，这样找到第二个分类器，同样的方法得到第三个分类器。现在我们有了三个分类器，接下来要做的是训练它们。在三个分类器里输入x，选择一个让h最大的类别。    

   ![](/images/images_2018/6-4_16.png)    

现在我们可以扩展我们的定义，当我们有两个以上的类别时y={0,1,...n}，我们将问题分为n+1二元分类问题，在每一个分类问题中去预测'y'是其中一个成员的概率，并选择那个概率最大的类别。   
  
   ![](/images/images_2018/6-4_17.png)     


## 逻辑回归测试题        

   ![](/images/images_2018/6-4_18.png)     

答案：A、C。y=1的概率是0.4，那么y=0的概率就是0.6。因此AC是正确的。  

   ![](/images/images_2018/6-4_19.png)     
 
   ![](/images/images_2018/6-4_19.jpg)        

答案：A、B。增加多项式特征，可以更好的拟合训练集是正确的；代价函数大于等于0是正确的；增加多项式特征可以更好的拟合训练集，因此代价函数的值不会变大，而是变小；无论梯度下降算法迭代多少次，假设函数的值是不会大于1的。因此AB是正确的，CD是错误的。     

   ![](/images/images_2018/6-4_20.png)    

答案：B、C。考察逻辑回归的梯度下降算法，与线性回归使用的算法相同。B和C是等价的，因此BC是正确的。       

   ![](/images/images_2018/6-4_21.png)    

答案： A、C。  S函数的范围是[0，1]，不会大于1；线性回归算法并不适合解决分类问题，其取值范围可能在[0,1]之外；逻辑回归的代价函数的取值是大于等于0的；使用高级优化算法可以大大提高进行逻辑回归的速度，更加适合解决大型的机器学习问题，逻辑回归采用log函数能解决其平方差代价函数不是凸函数的问题。因此AC是正确的，BD是错误的。       

   ![](/images/images_2018/6-4_22.png)      

   ![](/images/images_2018/6-4_23.png)  

答案：A。6-X1>=0时，y=1，即X1<=6时，y=1，因此A是正确的。      

## 过拟合问题和正则化技术   

我们使用那个用线性回归来预测房价的例子来解释什么是过度拟合问题。左图没有很好的拟合训练数据，我们把这个问题称之为欠拟合（underfitting），这个问题的另一个术语叫高偏差（bias），它的意思是，算法有非常大的偏差，可能是由于变量考虑不足或者对模型形式估计不足，该模型最终导致拟合数据效果差。    

右图的曲线通过了所有的训练实例，但这是一条扭曲的曲线，可能面临变量太多、函数太庞大的问题，这就是过度拟合（overfitting），或者叫做高方差（high variance）。这种模型总能很好的拟合训练集，但无法泛化到新的样本数据。“泛化”指的是一个假设模型能够应用到新样本的能力。       

中间的曲线尽管没有能够完美拟合所有的点，但基本能够把数据趋势很好地拟合出来，该模型在训练集上的误差比较低，并且有不错的泛化能力，这个状态相对合适。   

   ![](/images/images_2018/6-4_24.png)    

解决过拟合的方法：   

1、尽量减少选取特征变量的数量：人工选择，或者模型选择算法自动选择     
2、正则化（Regularization）：保留所有特征变量，但减小参数的数量级或值。这个方法非常有效，当我们有很多特征变量，并且其中每一个变量都是有用的。  

我们看到了用一个二次函数能很好的拟合数据，而用一个更高次的多项式时，它过度拟合了数据。我们的优化目标是尽量减小代价函数J。惩罚两个大的参数值的效果是最终得到了很好的一个二次函数，更加简单的函数，就不易发生过拟合的问题。  

   ![](/images/images_2018/6-4_25.png)    

当特征变量很多（比如100个）时，不知道选择哪些参数去缩小，就修改代价函数，添加一个额外的正则化项时，缩小了每个参数。    

   ![](/images/images_2018/6-4_26.png)    

如果正则化参数被设定为非常大，会发生什么呢？所有参数都接近于0，类似拟合了一条水平线，是欠拟合的情况。因此需要注意选择一个合适的正则化参数。         

   ![](/images/images_2018/6-4_27.png)      

## 正则化应用于线性回归算法   

梯度下降算法：   

   ![](/images/images_2018/6-4_28.png)     

也可以表示为:    

   ![](/images/images_2018/6-4_29.png)    

我们可以理解为把参数压缩了一点（比如原来的0.99倍），第二种算法的表达式和之前是一样的。  

正规方程：    

   ![](/images/images_2018/6-4_30.png)     
 

## 正则化应用于逻辑回归算法    

代价函数：   

   ![](/images/images_2018/6-4_31.png)       

梯度下降算法：   

   ![](/images/images_2018/6-4_32.png)      

## 正则化测试题   

   ![](/images/images_2018/6-4_33.png)    

答案： 

   ![](/images/images_2018/6-4_34.png)   

答案：   



     