---
layout:     post
title:      Machine Learning基础系列（二）
keywords:   博客
categories: [机器学习]
tags:	    [逻辑回归，正则化]
---

本文是机器学习第三周的课程内容。主要学习逻辑回归。逻辑回归是一种将数据分类为离散结果的方法。比如，我们可以使用逻辑回归将电子邮件分类为垃圾邮件或正常邮件。主要内容除了分类的概念，逻辑回归的代价函数，以及逻辑回归在多分类中的应用，还覆盖了正则化的概念。


## 分类      

我们先重点讨论二元分类问题，即y只能取两个值0和1，0也被称为负类，1是正类，有时也用符号“-”和“+”表示。要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，其它所有预测值映射为0。比如预测肿瘤是否是恶性还是良性的问题：   

  ![](/images/images_2018/6-4_0.png)       

我们的预测值只有在[0,1]才有意义，线性分类器则可能小于0或大于1。我们希望假设函数的预测值要在0和1之间，因此使用了S型函数（Sigmoid Function）或逻辑函数（Logistic Function）：    

 ![](/images/images_2018/6-4_1.png)      

从S型函数的图形看，它随着横坐标的正方向趋于1，随着横坐标的反方向趋于0。这个假设函数的输出值是对于输入样本x的y=1的概率的估计值。比如，以肿瘤分类为例，特征变量x是肿瘤的大小，假设x带入假设函数的输出为0.7，即肿瘤是恶性的可能性是70%或0.7。由于y必须是0或1，因此我们知道y=0的概率加上y=1的概率必须等于1。     

 ![](/images/images_2018/6-4_2.png)      


## 决策边界   

 ![](/images/images_2018/6-4_3.png)    

我们会注意到S函数只要z大于或等于0时，g(z)就将大于或等于0.5。如果我们默认预测y=1的概率大于或等于0.5时，y更有可能等于1而不是0，那么也意味着：   

 ![](/images/images_2018/6-4_4.png)    

决策边界将整个平面分成了两部分，其中一片区域假设函数预测y=1，另一片区域假设函数预测y=0。决策边界是假设函数的一个属性，一旦有确定的假设函数的参数，就能完全确定决策边界。    

  ![](/images/images_2018/6-4_5.png)   

在这个例子中决策边界是一条竖线，左边的表示y=1，右边的则表示y=0。 决策边界有时并不是一条直线，它可以是任意形状的，比如圆形。

   ![](/images/images_2018/6-4_6.png)    

## 代价函数    
 
我们不能使用和线性回归相同的代价函数，因为逻辑函数不是一个凸函数，会导致许多局部最优。但是以下面这种方式编写代价函数可以保证是凸函数。        

  ![](/images/images_2018/6-4_7.png)        

  ![](/images/images_2018/6-4_8.png)     

  ![](/images/images_2018/6-4_9.png)     

y=0时，如果假设函数也输出0，则代价函数为0，如果假设函数接近1，则代价函数将接近无穷大；
y=1时，如果假设函数也输出1，则代价函数为0，如果假设函数接近0，则代价函数将接近无穷大。   

  ![](/images/images_2018/6-4_10.png)


## 简化的代价函数和梯度下降算法   

将y=0和y=1两种情况下的表达式压缩成一个表达式：   
 
  ![](/images/images_2018/6-4_11.png)        

完整的代价函数J的表达式：   

  ![](/images/images_2018/6-4_12.png)     

代价函数向量化的表达式：    
 
   ![](/images/images_2018/6-4_13.png)     

逻辑回归的梯度下降算法与线性回归中使用的算法相同，仍然需要同时更新theta中的所有值：   
 
   ![](/images/images_2018/6-4_14.png)        

向量化的实现方式：    

   ![](/images/images_2018/6-4_15.png)  

   




  
  
