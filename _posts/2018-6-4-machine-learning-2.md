---
layout:     post
title:      Machine Learning基础系列（二）
keywords:   博客
categories: [机器学习]
tags:	    [逻辑回归，正则化]
---

本文是机器学习第三周的课程内容。主要学习逻辑回归。逻辑回归是一种将数据分类为离散结果的方法。比如，我们可以使用逻辑回归将电子邮件分类为垃圾邮件或正常邮件。主要内容除了分类的概念，逻辑回归的代价函数，以及逻辑回归在多分类中的应用，还覆盖了正则化的概念。


## 分类      

我们先重点讨论二元分类问题，即y只能取两个值0和1，0也被称为负类，1是正类，有时也用符号“-”和“+”表示。要尝试分类，一种方法是使用线性回归，并将所有大于0.5的预测值映射为1，其它所有预测值映射为0。比如预测肿瘤是否是恶性还是良性的问题：   

  ![](/images/images_2018/6-4_0.png)       

我们的预测值只有在[0,1]才有意义，线性分类器则可能小于0或大于1。我们希望假设函数的预测值要在0和1之间，因此使用了S型函数（Sigmoid Function）或逻辑函数（Logistic Function）：    

 ![](/images/images_2018/6-4_1.png)      

从S型函数的图形看，它随着横坐标的正方向趋于1，随着横坐标的反方向趋于0。这个假设函数的输出值是对于输入样本x的y=1的概率的估计值。比如，以肿瘤分类为例，特征变量x是肿瘤的大小，假设x带入假设函数的输出为0.7，即肿瘤是恶性的可能性是70%或0.7。由于y必须是0或1，因此我们知道y=0的概率加上y=1的概率必须等于1。     

 ![](/images/images_2018/6-4_2.png)      


## 决策边界   

 ![](/images/images_2018/6-4_3.png)    

我们会注意到S函数只要z大于或等于0时，g(z)就将大于或等于0.5。如果我们默认预测y=1的概率大于或等于0.5时，y更有可能等于1而不是0，那么也意味着：   

 ![](/images/images_2018/6-4_4.png)    

决策边界将整个平面分成了两部分，其中一片区域假设函数预测y=1，另一片区域假设函数预测y=0。决策边界是假设函数的一个属性，一旦有确定的假设函数的参数，就能完全确定决策边界。    

  ![](/images/images_2018/6-4_5.png)   

在这个例子中决策边界是一条竖线，左边的表示y=1，右边的则表示y=0。 决策边界有时并不是一条直线，它可以是任意形状的，比如圆形。

   ![](/images/images_2018/6-4_6.png)    

## 线性回归算法 
 
课程以预测房价为例，引入了线性回归算法。假设我们有一个训练集，包含不同房屋的面积和价格，我们要基于房屋面积来预测房屋价格。这意味着我们要预测一个关于X的线性函数y,为什么是线性函数呢，因为它是简单的形式，我们先从线性方程入手，最终将建立更复杂的模型。   

  ![](/images/images_2018/week1_0.png)   

我们要寻找最适合训练集的假设h，h是一个x到y的函数：  

  ![](/images/images_2018/week1_1.png)     
  
## 代价函数    

在线性回归算法中，h是一条直线。在这个模型中有两个参数值，选择不同的参数会得到不同的假设。我们要选择的参数值，要让h表示的直线尽量地和这些数据点很好的拟合，即让h(x)和y之间的差异最小化。这个函数就是代价函数(Cost function)：   

  ![](/images/images_2018/week1_2.png)   

代价函数也被称为平方误差函数，这所以选择这个函数，是因为对于大多数问题，特别是回归问题，平方误差代价函数是非常合理的选择，也是最常用的手段。   

## 梯度下降算法   

利用梯度下降算法可以将代价函数最小化。通常的选择是将两个参数初始化为0，然后一点点改变两个参数的值，使得代价函数J变小，直到找到J的最小值或局部最小值。下图中，两个参数分别对应x,y轴，J对应z轴，我们把它想象成一座山，如果想尽快下山，就要选择一个能最快下山的方向迈进一小步，每一步都依次类推，直到局部最低点的位置。起始位置不同，可能得到完全不同的结果。

  ![](/images/images_2018/week1_3.jpg)     

梯度下降算法的公式如下：      

  ![](/images/images_2018/week1_4.png)      

如何理解通过梯度下降算法我们可以得到代价函数的最小值呢？这里涉及到一些微积分的基础概念，让我们简单回顾一下。导数描述了函数在某点的瞬时变化率，而偏导数则为函数在某点沿着坐标轴方向上的变化率。以下图为例，刚好与函数曲线的某点相切的这条直线的斜率就是导数。正斜率意味着正导数，会减去某个值，使参数变小，更接近最低点；负斜率意味着负导数，会加上某个值，使参数变大，也是更接近最低点。    

  ![](/images/images_2018/week1_5.png)       

在梯度下降算法中还有一个参数被称为学习速率，它控制了我们下山会迈出多大的步子。如果学习速率太小，需要很多步才能达到最低点；如果学习速率太大，可能一次次越过最低点，会导致无法收敛，甚至发散。如果参数的初始值已经是局部最优点，由于斜率为0，梯度下降算法不会改变参数的值，会保持在局部最优点。    
  
  ![](/images/images_2018/week1_6.png)       

在梯度下降算法中，当我们接近局部最低点时，梯度下降算法会自动采取更小的幅度，因为这时导数值会自动变得越来越小。即使学习速率保持不变，梯度下降也可以收敛到局部最低点。
   
  ![](/images/images_2018/week1_7.png)       

## 线性回归的梯度下降算法  

这里直接给出线性回归的偏导数的计算公式：       

  ![](/images/images_2018/week1_8.png)      

执行梯度下降算法时，有一个细节需要注意，就是需要同时更新两个参数。由于线性回归的代价函数是一个凸函数，因此这个函数没有局部最优解，只有一个全局最优解。       

## 单变量的线性回归的练习题    


 ![](/images/images_2018/week1_11.png)       

答案：4。考察的是训练集的概念。   

   

 ![](/images/images_2018/week1_12.jpg)  

答案：D。考察代价函数的概念。   

 

 ![](/images/images_2018/week1_13.png)   

答案：11。考察假设函数的概念，直接带入x=6求值即可。   

   

 ![](/images/images_2018/week1_14.png)     

答案：B、D。如果学习速率太大，会不收敛甚至发散，所以A错了；两个参数的偏导数公式是不同的，因此即使参数相同，同步更新后的值也是不同的。

 

 ![](/images/images_2018/week1_15.png)      

答案：B。线性回归的代价函数为0只是意味着直线完全拟合测试集的数据（h-y=0），并不意味y或h为0，A和C错了；线性回归的代价函数是凸函数，没有局部最优，只有全局最优，D错了。      


## 多元线性回归  

在之前的线性回归中，我们只有一个特征量 房屋面积，当我们有更多特征量来预测房屋价格时，我们可以这样来表示：

   ![](/images/images_2018/week2_1.png)    

我们用矩阵乘法的方式简化上面这个等式的表示方式：  

   ![](/images/images_2018/week2_2.png)   

我们定义了一个额外的第0个特征量，并且它的取值总是1，所以现在的特征向量是0开始的n+1维的向量。这种多特征量情况下的假设形式，就是所谓的多元线性回归。    

## 使用梯度下降法解决多元线性回归问题    

多元线性回归的梯度下降算法和单变量线性回归的梯度下降算法实际上是一回事儿，两个算法的对比图如下：         

  ![](/images/images_2018/week2_3.png)             

一种更紧凑的表示方式：        

  ![](/images/images_2018/week2_4.png)      

## 特征缩放      

为什么要进行特征缩放？如果能保证多个特征都在一个相近的范围，那么梯度下降算法能更快地收敛（可以从数学上证明）。比较理想的范围是：     

 ![](/images/images_2018/week2_5.png)         

范围如果大于-3到+3的范围就要开始注意了。在特征缩放中，将特征值除以最大值外，还有一个被称为均值归一化的工作（mean normalization）。   

 ![](/images/images_2018/week2_6.png)      

举个例子，房价的范围是100到2000，平均值是1000，那么均值归一化的公式是：(price-1000)/1900。     

## 如何选择学习速率      

我们可以在梯度下降算法运行时画一条曲线：X轴表示梯度下降算法的迭代步数，y轴表示代价函数的值，当曲线已经很平坦了，没有继续下降，说明算法基本已经收敛了。如果学习速率足够小，那么代价函数的值在每次迭代中都会下降（已被证明）；如果学习速率太小，收敛会很慢；如果学习速率太大，则无法保证每次迭代都下降，也就是说不收敛。 下图中的三种情况都是不收敛的，需要使用更小的学习速率。     

 ![](/images/images_2018/week2_7.png)     

## 正规方程   

对于某些线性回归问题，用正规方程法(Normal Equation)求解参数的最优值更好。不同于梯度下降算法需要通过多次迭代直至收敛到全局最小值，正规方程法可以直接一次性求解。正规方程法不需要进行特征缩放。      
 
 ![](/images/images_2018/week2_8.png)        

对比梯度下降法和正规方程法的优缺点，我们可以得到一个结论：只要特征变量的数目并不大（n<10000），正规方程是一个很好的计算参数的替代方法，当n上万时（n>10000），应该考虑换成梯度下降法。对于更复杂的一些学习算法，并不能使用正规方程法，仍然不得不使用梯度下降法，因此梯度下降法是一个非常有用的算法。      

 ![](/images/images_2018/week2_9.png)   

## 正规方程的不可逆性      

正规方程不可逆基本有两个原因：   

一是有多余的特征变量，比如，它们是线性关联的；二是有大量的特征变量，比如样本数少于特征变量数。解决方法是去掉多余的特征，删除一些特征变量或者使用一种正则化的线性代数方法。       

  ![](/images/images_2018/week2_10.png)              

##  多元线性回归的练习题     

  ![](/images/images_2018/week2_11.png)           

答案：0.32。这题考察的是均值归一化的概念。        

  ![](/images/images_2018/week2_12.png)       
       
答案：B。代价函数不收敛需要减少学习速率。        

  ![](/images/images_2018/week2_13.png)             

答案：C。注意要增加一个额外的特征变量。        

  ![](/images/images_2018/week2_14.png)            

答案：B。 n=200000远大于10000，如果使用正规方程法，计算会非常慢，因此应使用梯度下降法。        

  ![](/images/images_2018/week2_15.png)      
    
答案：B。正规方程法不需要特征缩放，所以C、D是错误的，特征缩放可以帮助梯度下降算法更快收敛，即减少迭代次数，无关单次迭代的复杂度，所以A也是错误的，只有B是正确的选项。    
 

