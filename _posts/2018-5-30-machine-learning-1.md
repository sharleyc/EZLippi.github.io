---
layout:     post
title:      Machine Learning基础系列（一）
keywords:   博客
categories: [Python]
tags:	    [ml]
---

2018年上半年自学了吴恩达在coursera上的machine learning网上课程。这个博客系列融合了该课程的读书笔记以及在python上的实践。我学习该课程的初衷是希望在机器学习上有个入门，对机器学习的基本概念和原理有所了解。从这个角度讲，该课程确实值得推荐，但如果你想学完这门课程就能直接做项目，还是远远不够的。

## 机器学习的定义    

课程中对机器学习给出了两个定义：     
第一个定义来自于Arthur Samuel（1959）。他定义机器学习为：在进行特定编程的情况下，给予计算机学习能力的领域   
（Field of study that gives computers the ability to learn without being explicitly programmed）    

另一个年代近一点的定义，是卡内基梅隆大学的Tom Mitchell提出的，其定义如下， 一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序在处理T时的性能有所提升。   
（A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P,if its performance at tasks in T,as measured by P,improves with experience E）     

## 机器学习的分类    

广义上分为监督学习和非监督学习（Supervised learning and Unsupervised learning）    

监督学习，意指给出一个算法，需要部分数据集已经有正确答案。监督学习大致可划分为两类：回归和分类，前者是要预测连续的值，比如根据照片预测人的年龄，后者是预测离散的值，比如确定病人的肿瘤是良性的还是恶性的。     

无监督学习是一种学习机制，你给算法大量的数据，要求它找出数据中蕴含的类型结构。比如自动发现细分市场。无监督学习大致可分为两类：聚类和非聚类（鸡尾酒会算法）。

## 线性回归算法 
 
第一周的课程里介绍了第一种机器学习算法：线性回归算法。以预测房价为例，引入了线性回归算法。我们要寻找最适合训练集的假设h，h是一个x到y的函数映射。

  ![](/images/images_2018/week1_1.png)     
  
## 代价函数    

在线性回归算法中，h是一条直线。在这个模型中有两个参数值，选择不同的参数会得到不同的假设。我们要选择的参数值，要让h表示的直线尽量地和这些数据点很好的拟合，即让h(x)和y之间的差异最小化。这个函数就是代价函数(Cost function)：   

  ![](/images/images_2018/week1_2.png)   

代价函数也被称为平方误差函数，这所以选择这个函数，是因为对于大多数问题，特别是回归问题，平方误差代价函数是非常合理的选择，也是最常用的手段。   

## 梯度下降算法   

利用梯度下降算法可以将代价函数最小化。通常的选择是将两个参数初始化为0，然后一点点改变两个参数的值，使得代价函数J变小，直到找到J的最小值或局部最小值。下图中，两个参数分别对应x,y轴，J对应z轴，我们把它想象成一座山，如果想尽快下山，就要选择一个能最快下山的方向迈进一小步，每一步都依次类推，直到局部最低点的位置。起始位置不同，可能得到完全不同的结果。

  ![](/images/images_2018/week1_3.jpg)     

梯度下降算法的公式如下：      

  ![](/images/images_2018/week1_4.png)      

如何理解通过梯度下降算法我们可以得到代价函数的最小值呢？这里涉及到一些微积分的基础概念，让我们简单回顾一下。导数描述了函数在某点的瞬时变化率，而偏导数则为函数在某点沿着坐标轴方向上的变化率。以下图为例，刚好与函数曲线的某点相切的这条直线的斜率就是导数。正斜率意味着正导数，会减去某个值，使参数变小，更接近最低点；负斜率意味着负导数，会加上某个值，使参数变大，也是更接近最低点。    

  ![](/images/images_2018/week1_5.png)       

在梯度下降算法中还有一个参数被称为学习速率，它控制了我们下山会迈出多大的步子。如果学习速率太小，需要很多步才能达到最低点；如果学习速率太大，可能一次次越过最低点，会导致无法收敛，甚至发散。如果参数的初始值已经是局部最优点，由于斜率为0，梯度下降算法不会改变参数的值，会保持在局部最优点。    
  
  ![](/images/images_2018/week1_6.png)       

在梯度下降算法中，当我们接近局部最低点时，梯度下降算法会自动采取更小的幅度，因为这时导数值会自动变得越来越小。即使学习速率保持不变，梯度下降也可以收敛到局部最低点。
   
  ![](/images/images_2018/week1_7.png)       

## 线性回归的梯度下降算法  

这里直接给出线性回归的偏导数的计算公式：       

  ![](/images/images_2018/week1_8.png)      

执行梯度下降算法时，有一个细节需要注意，就是需要同时更新两个参数。由于线性回归的代价函数是一个凸函数，因此这个函数没有局部最优解，只有一个全局最优解。       

## 单变量的线性回归的练习题    


 ![](/images/images_2018/week1_11.png)       

答案：4。考察的是训练集的概念。   

   

 ![](/images/images_2018/week1_12.jpg)  

答案：D。考察代价函数的概念。   

 

 ![](/images/images_2018/week1_13.png)   

答案：11。考察假设函数的概念，直接带入x=6求值即可。   

   

 ![](/images/images_2018/week1_14.png)     

答案：B、D。如果学习速率太大，会不收敛甚至发散，所以A错了；两个参数的偏导数公式是不同的，因此即使参数相同，同步更新后的值也是不同的。

 

 ![](/images/images_2018/week1_15.png)      

答案：B。线性回归的代价函数为0只是意味着直线完全拟合测试集的数据（h-y=0），并不意味y或h为0，A和C错了；线性回归的代价函数是凸函数，没有局部最优，只有全局最优，D错了。     
 

