---
layout:     post
title:      Machine Learning基础系列（三）
keywords:   博客
categories: [机器学习]
tags:	    [神经网络]
---

本文是机器学习第四周的课程内容，主要学习神经网络（Neural Network）。神经网络是受大脑如何工作启发的模型。它被广泛应用于现在的许多应用中：语音生成和识别、图像和物体识别和电子游戏等。


## 为什么需要神经网络      

许多实际的机器学习问题，特征个数n是很大的，以计算机视觉中的图像识别为例。我们自己看是一目了然的事情，但是算法不知道图像是什么。      

  ![](/images/images_2018/6-6_1.png)    

计算机看到的是一个数据矩阵。假设我们用50*50像素的图片，那么一共有2500个像素点，因此特征向量的元素数量n=2500。如果我们用RGB彩色图像，每个像素点包含红、绿、蓝三个子像素，那么特征向量的元素数量n=7500。如果通过逻辑回归算法（包含所有的二次项）来解决这个问题，那么需要大约300万个项，计算成本太高了。而神经网络在解决复杂的非线性分类问题上，被证明是一种好得多的算法，即使特征维数n很大也能轻松搞定。        

 ![](/images/images_2018/6-6_2.png)      


## 神经网络模型

神经网络是在模仿大脑中的神经元或神经网络时发明的。单个神经元由细胞主体、树突(dendrites)和轴突(axons)组成。简言之，神经元是一个计算单元，它从树突接受一定信息并做一些计算后，将结果通过轴突传送到其它神经元。    

 ![](/images/images_2018/6-6_3.jpg)    
   
我们在电脑上实现的人工神经网络里，将神经元模拟成一个逻辑单元，把这样的黄色圆圈想象成类似神经元的东西，输入节点是x1、x2、x3，有时会增加一个额外的节点x0，也被称作偏置单元（bias unit），偏置单元的值永远是1。图中是一个有S型函数作为激励函数的人工神经元，激励函数是对类似非线性函数的一个术语称呼。模型的参数("theta")有时也被称为权重（"weights"）。

 ![](/images/images_2018/6-6_4.png)    

决策边界将整个平面分成了两部分，其中一片区域假设函数预测y=1，另一片区域假设函数预测y=0。决策边界是假设函数的一个属性，一旦有确定的假设函数的参数，就能完全确定决策边界。    

  ![](/images/images_2018/6-4_5.png)   

在这个例子中决策边界是一条竖线，左边的表示y=1，右边的则表示y=0。 决策边界有时并不是一条直线，它可以是任意形状的，比如圆形。

   ![](/images/images_2018/6-4_6.png)    

## 代价函数    
 
我们不能使用和线性回归相同的代价函数，因为逻辑函数不是一个凸函数，会导致许多局部最优。但是以下面这种方式编写代价函数可以保证是凸函数。        

  ![](/images/images_2018/6-4_7.png)        

  ![](/images/images_2018/6-4_8.png)     

  ![](/images/images_2018/6-4_9.png)     

y=0时，如果假设函数也输出0，则代价函数为0，如果假设函数接近1，则代价函数将接近无穷大；
y=1时，如果假设函数也输出1，则代价函数为0，如果假设函数接近0，则代价函数将接近无穷大。   

  ![](/images/images_2018/6-4_10.png)


## 简化的代价函数和梯度下降算法   

将y=0和y=1两种情况下的表达式压缩成一个表达式：   
 
  ![](/images/images_2018/6-4_11.png)        

完整的代价函数J的表达式：   

  ![](/images/images_2018/6-4_12.png)     

代价函数向量化的表达式：    
 
   ![](/images/images_2018/6-4_13.png)     

逻辑回归的梯度下降算法与线性回归中使用的算法相同，仍然需要同时更新theta中的所有值：   
 
   ![](/images/images_2018/6-4_14.png)        

向量化的实现方式：    

   ![](/images/images_2018/6-4_15.png)  

   
## 多类别分类问题（One-vs-all）    

我们通过一个例子来了解什么是多类别分类问题。现在我们有一个训练集，有三个类别，三角形表示y=1，方框表示y=2，叉叉表示y=3。我们将它分成三个二元分类问题。先从三角形的代表的类别1开始。类别1是正类，类别2和3是负类，我们要拟合出一个合适的逻辑回归分类器，接着将为类别2做同样的事，把方块类当做正样本，其它类别为负样本，这样找到第二个分类器，同样的方法得到第三个分类器。现在我们有了三个分类器，接下来要做的是训练它们。在三个分类器里输入x，选择一个让h最大的类别。    

   ![](/images/images_2018/6-4_16.png)    

现在我们可以扩展我们的定义，当我们有两个以上的类别时y={0,1,...n}，我们将问题分为n+1二元分类问题，在每一个分类问题中去预测'y'是其中一个成员的概率，并选择那个概率最大的类别。   
  
   ![](/images/images_2018/6-4_17.png)     


## 逻辑回归测试题        

   ![](/images/images_2018/6-4_18.png)     

答案：A、C。y=1的概率是0.4，那么y=0的概率就是0.6。因此AC是正确的。  

   ![](/images/images_2018/6-4_19.png)     
 
   ![](/images/images_2018/6-4_19.jpg)        

答案：A、B。增加多项式特征，可以更好的拟合训练集是正确的；代价函数大于等于0是正确的；增加多项式特征可以更好的拟合训练集，因此代价函数的值不会变大，而是变小；无论梯度下降算法迭代多少次，假设函数的值是不会大于1的。因此AB是正确的，CD是错误的。     

   ![](/images/images_2018/6-4_20.png)    

答案：B、C。考察逻辑回归的梯度下降算法，与线性回归使用的算法相同。B和C是等价的，因此BC是正确的。       

   ![](/images/images_2018/6-4_21.png)    

答案： A、C。  S函数的范围是[0，1]，不会大于1；线性回归算法并不适合解决分类问题，其取值范围可能在[0,1]之外；逻辑回归的代价函数的取值是大于等于0的；使用高级优化算法可以大大提高进行逻辑回归的速度，更加适合解决大型的机器学习问题，逻辑回归采用log函数能解决其平方差代价函数不是凸函数的问题。因此AC是正确的，BD是错误的。       

   ![](/images/images_2018/6-4_22.png)      

   ![](/images/images_2018/6-4_23.png)  

答案：A。6-X1>=0时，y=1，即X1<=6时，y=1，因此A是正确的。      

## 过拟合问题和正则化技术   

我们使用那个用线性回归来预测房价的例子来解释什么是过度拟合问题。左图没有很好的拟合训练数据，我们把这个问题称之为欠拟合（underfitting），这个问题的另一个术语叫高偏差（bias），它的意思是，算法有非常大的偏差，可能是由于变量考虑不足或者对模型形式估计不足，该模型最终导致拟合数据效果差。    

右图的曲线通过了所有的训练实例，但这是一条扭曲的曲线，可能面临变量太多、函数太庞大的问题，这就是过度拟合（overfitting），或者叫做高方差（high variance）。这种模型总能很好的拟合训练集，但无法泛化到新的样本数据。“泛化”指的是一个假设模型能够应用到新样本的能力。       

中间的曲线尽管没有能够完美拟合所有的点，但基本能够把数据趋势很好地拟合出来，该模型在训练集上的误差比较低，并且有不错的泛化能力，这个状态相对合适。   

   ![](/images/images_2018/6-4_24.png)    

解决过拟合的方法：   

1、尽量减少选取特征变量的数量：人工选择，或者模型选择算法自动选择     
2、正则化（Regularization）：保留所有特征变量，但减小参数的数量级或值。这个方法非常有效，当我们有很多特征变量，并且其中每一个变量都是有用的。  

我们看到了用一个二次函数能很好的拟合数据，而用一个更高次的多项式时，它过度拟合了数据。我们的优化目标是尽量减小代价函数J。惩罚两个大的参数值的效果是最终得到了很好的一个二次函数，更加简单的函数，就不易发生过拟合的问题。  

   ![](/images/images_2018/6-4_25.png)    

当特征变量很多（比如100个）时，不知道选择哪些参数去缩小，就修改代价函数，添加一个额外的正则化项时，缩小了每个参数。    

   ![](/images/images_2018/6-4_26.png)    

如果正则化参数被设定为非常大，会发生什么呢？所有参数都接近于0，类似拟合了一条水平线，是欠拟合的情况。因此需要注意选择一个合适的正则化参数。         

   ![](/images/images_2018/6-4_27.png)      

## 正则化应用于线性回归算法     

代价函数：   

   ![](/images/images_2018/6-4_28_J.png) 

梯度下降算法：     

   ![](/images/images_2018/6-4_28.png)         

也可以表示为:    

   ![](/images/images_2018/6-4_29.png)    

我们可以理解为把参数压缩了一点（比如原来的0.99倍），第二种算法的表达式和之前是一样的。  

正规方程：    

   ![](/images/images_2018/6-4_30.png)     
 

## 正则化应用于逻辑回归算法    

代价函数：   

   ![](/images/images_2018/6-4_31.png)       

梯度下降算法：   

   ![](/images/images_2018/6-4_32.png)      

## 正则化测试题   

   ![](/images/images_2018/6-4_33.png)    

答案：C。正则化参数过大会导致欠拟合，无论是训练集还是非训练集的效果都不好，因此A和D是错误的；增加特征变量会提高训练集上的拟合度，而不是example的拟合度。因此B是错误的；增加过多的特征变量会导致训练集上过拟合，因此C是正确的。       

   ![](/images/images_2018/6-4_34.png)   

答案：A。逻辑回归算法中的正则化参数会让方程参数变小，因此选择数值较小的那组参数。    

   ![](/images/images_2018/6-4_35.png) 

答案：C。正则化参数过大会导致欠拟合，因此A和B是错误的；正则化可以减少过拟合的风险，同时也意味着会降低模型在训练集上的效果，因此C是正确的；正则化对于线性回归和逻辑回归都可以减少过拟合的风险，因此D是错误的。     

   ![](/images/images_2018/6-4_36.png)    

   ![](/images/images_2018/6-4_36_A.png) 

   ![](/images/images_2018/6-4_36_B.png) 

   ![](/images/images_2018/6-4_36_C.png) 

   ![](/images/images_2018/6-4_36_D.png)   

答案：A。A是过拟合，D是欠拟合，B和C是刚好。      

   ![](/images/images_2018/6-4_37.png)    

   ![](/images/images_2018/6-4_37_A.png) 

   ![](/images/images_2018/6-4_37_B.png) 

   ![](/images/images_2018/6-4_37_C.png) 

   ![](/images/images_2018/6-4_37_D.png)   

答案：A。A是欠拟合，B是过拟合，C和D是刚好。    

     