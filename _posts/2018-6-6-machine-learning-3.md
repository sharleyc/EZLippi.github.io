---
layout:     post
title:      Machine Learning基础系列（三）
keywords:   博客
categories: [机器学习]
tags:	    [神经网络]
---

本文是机器学习第四周的课程内容，主要学习神经网络（Neural Network）。神经网络是受大脑如何工作启发的模型。它被广泛应用于现在的许多应用中：语音生成和识别、图像和物体识别和电子游戏等。


## 为什么需要神经网络      

许多实际的机器学习问题，特征个数n是很大的，以计算机视觉中的图像识别为例。我们自己看是一目了然的事情，但是算法不知道图像是什么。      

  ![](/images/images_2018/6-6_1.png)    

计算机看到的是一个数据矩阵。假设我们用50*50像素的图片，那么一共有2500个像素点，因此特征向量的元素数量n=2500。如果我们用RGB彩色图像，每个像素点包含红、绿、蓝三个子像素，那么特征向量的元素数量n=7500。如果通过逻辑回归算法（包含所有的二次项）来解决这个问题，那么需要大约300万个项，计算成本太高了。而神经网络在解决复杂的非线性分类问题上，被证明是一种好得多的算法，即使特征维数n很大也能轻松搞定。        

 ![](/images/images_2018/6-6_2.png)      


## 神经网络模型

神经网络是在模仿大脑中的神经元或神经网络时发明的。单个神经元由细胞主体、树突(dendrites)和轴突(axons)组成。简言之，神经元是一个计算单元，它从树突接受一定信息并做一些计算后，将结果通过轴突传送到其它神经元。    

 ![](/images/images_2018/6-6_3.jpg)    
   
我们在电脑上实现的人工神经网络里，将神经元模拟成一个逻辑单元，把这样的黄色圆圈想象成类似神经元的东西，输入节点是x1、x2、x3，有时会增加一个额外的节点x0，也被称作偏置单元（bias unit），偏置单元的值永远是1。图中是一个有S型函数作为激励函数的人工神经元，激励函数是对类似非线性函数的一个术语称呼。模型的参数("theta")有时也被称为权重（"weights"）。

 ![](/images/images_2018/6-6_4.png)    

神经网络的术语中，网络中的第一层被称为输入层，最后一层被称为输出层，非输入层或非输出层就被称为隐藏层。a上标(2)下标1 表示第二层的第一个激励，所谓激励（activation）是指由一个具体神经元读入计算并输出的值。如何计算它的值：激励函数作用在输入的线性组合上的结果。这样一来，参数矩阵控制了来自三个输入单元，三个隐藏单元的映射。参数矩阵的大小：Sj+1 x (Sj + 1)，Sj = 3, Sj+1 = 3，则对应的参数矩阵为3 x 4。    

  ![](/images/images_2018/6-6_5.jpg)     

 

## 向量化的实现方法     
 
下面是一个神经网络的例子：           

  ![](/images/images_2018/6-6_6.png)    

  ![](/images/images_2018/6-6_6.jpg)       
    

为了实现向量化，我们引入了新变量z：      

  ![](/images/images_2018/6-6_7.png)   

  ![](/images/images_2018/6-6_8.png)     

x和z的向量化表示：   

  ![](/images/images_2018/6-6_9.png)  

  ![](/images/images_2018/6-6_10.png)         

现在我们可以得到第j层的激励：    

  ![](/images/images_2018/6-6_11.png)   

为了计算假设的实际输出，只需要计算：

  ![](/images/images_2018/6-6_12.png)       

这个从输入层到隐藏层再到输出层的依次计算激励的过程叫前向传播。值得注意的是，在最后一步，在第j层和第j+1层之间，我们做的是和逻辑回归中完全相同的事情。在神经网络中添加中间层，有助于我们更优雅地产生有趣和更复杂的非线性假设。     

  ![](/images/images_2018/6-6_13.png)    

## 神经网络的实例讲解   

前面的内容比较抽象，我们具体的例子来了解，为什么神经网络可以用来学习复杂的非线性假设。  

我们应有神经网络来预测 x1 AND x2的值。方程和参数矩阵：     

  ![](/images/images_2018/6-6_14.png)    
  ![](/images/images_2018/6-6_15.png)    

假设输出结果：   

  ![](/images/images_2018/6-6_16.png)     
 
  ![](/images/images_2018/6-6_17.jpg)    

解释一下真值表的计算方法，S型函数如果横轴值等于4.6，则值等于0.99，非常接近1；横轴值为-4.6，则值等于0.01，非常接近0。

  ![](/images/images_2018/6-6_18.png)    

我们通过使用小型神经网络模拟了计算机中的基本操作之一的AND。神经网络同样可以模拟其它逻辑操作，比如AND,NOR和OR：   

  ![](/images/images_2018/6-6_19.png)     

把上面三个网络放在一起，增加一个包含两个节点的隐藏层，我们就能计算XNOR运算。   

  ![](/images/images_2018/6-6_20.png)       

第一层和第二层之间的参数矩阵：  
  
  ![](/images/images_2018/6-6_21.png)    

第二层和第三层之间的参数矩阵：  

   ![](/images/images_2018/6-6_22.png)    

所有节点的求值公式：       

   ![](/images/images_2018/6-6_23.png)       