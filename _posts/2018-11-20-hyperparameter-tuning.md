---
layout:     post
title:      超参数调优的指导原则和技巧
keywords:   博客
categories: [机器学习]
tags:	    [超参数，调优]
---

在深度神经网络训练中，包含了对很多不同超参数的设置，本文整理记录了一些指导原则和系统化调优的技巧。

## 超参数重要性排序

   ![](/images/images_2018/11-20_01.png)

 - 最重要的：
   - 学习率α
 - 其次重要的：
   - 动量项，0.9是不错的默认值
   - Mini-batch大小
   - 隐藏单元数量
 - 第三重要的：
   - 网络层数
   - 学习率衰减
 - Adam优化算法采用默认参数：
   - β1: 0.9
   - β2: 0.999
   - epsilon: 10^(-8)



## 如何选择超参数组合

### 在网格中随机取样

   ![](/images/images_2018/11-20_02.png)
   
调参的真正困难在于，你需要事先知道哪一个超参数对于你的模型更重要。随机取样能更充分的为最重要的超参数尝试尽可能多的组合。(无论最重要的超参数是哪个)


### 区域定位的抽样方案

   ![](/images/images_2018/11-20_03.png)

在这个方案中，需要你能大体确定一个最优区域，即最理想的超参数来自于这个区域。然后你就可以在更小的方块内进行更密集的抽样。


## 选取合适的尺度

均匀随机抽样在某些情况下是合理的方案，但它并不对所有的超参数都适用。

### 学习率的方案

   ![](/images/images_2018/11-20_04.png)

假设你认为学习率的下限是0.0001，上限是1，画出0.0001~1的数轴，如果均匀随机抽样，90%的
样本值将落在0.1~1的范围内。更合理的方法，应该是对数尺度搜索，而不是线性尺度。用python实现就是：

     r=-4*np.random.rand()

r为-4~0的随机数，则alpha值在10^(-4)~10^0之间。
